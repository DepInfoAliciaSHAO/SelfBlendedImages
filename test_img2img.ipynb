{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b31ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Charger une image\n",
    "def load_image(path, size=512):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img = img.resize((size, size))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f6fbb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.002509593963623047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading pipeline components...",
       "rate": null,
       "total": 7,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2503ab6c0ccc45159b359a9c97ac8988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append(\"../../facefusion/facefusion\")\n",
    "\n",
    "# Initialiser le pipeline\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58ffd301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0019342899322509766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 250,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f1d240baeb4855a591f56673b27509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0016124248504638672,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 250,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b801fa871d934a99a5a9cadbde15b73b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Paramètres\n",
    "prompt = \"a realistic photo of a person\"  # Le prompt peut être générique\n",
    "strength = 0.05  # Très faible pour ne PAS altérer le contenu\n",
    "guidance_scale = 1.0  # Faible aussi pour limiter le changement\n",
    "\n",
    "origin_image = load_image(\"../../extracted_frames/alexandre.jpg\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    result1 = pipe(\n",
    "        prompt=prompt,\n",
    "        image=origin_image,\n",
    "        strength=strength,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=5000\n",
    "    ).images[0]\n",
    "\n",
    "transf_image = load_image(\"../../extracted_frames_output/alexandre_florian_simswap_256_gfpgan_1.4_75_0.7.jpg\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    result2 = pipe(\n",
    "        prompt=prompt,\n",
    "        image=transf_image,\n",
    "        strength=strength,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=5000\n",
    "    ).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc296365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alex\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/alex/miniconda3/envs/env_sbi/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "LPIPS score: 0.004460480529814959\n",
      "LPIPS score: 0.0039420113898813725\n",
      "vgg\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /home/alex/miniconda3/envs/env_sbi/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth\n",
      "LPIPS score: 0.017657753080129623\n",
      "LPIPS score: 0.016568021848797798\n"
     ]
    }
   ],
   "source": [
    "import lpips\n",
    "import torchvision.transforms as T\n",
    "\n",
    "print(\"alex\")\n",
    "loss_fn = lpips.LPIPS(net='alex')  # ou 'vgg'\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "img1 = transform(origin_image).unsqueeze(0)\n",
    "img2 = transform(result1).unsqueeze(0)\n",
    "score = loss_fn(img1, img2)\n",
    "print(f\"LPIPS score: {score.item()}\")\n",
    "\n",
    "img1 = transform(transf_image).unsqueeze(0)\n",
    "img2 = transform(result2).unsqueeze(0)\n",
    "score = loss_fn(img1, img2)\n",
    "print(f\"LPIPS score: {score.item()}\")\n",
    "\n",
    "print(\"vgg\")\n",
    "loss_fn = lpips.LPIPS(net='vgg')  # ou 'vgg'\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "img1 = transform(origin_image).unsqueeze(0)\n",
    "img2 = transform(result1).unsqueeze(0)\n",
    "score = loss_fn(img1, img2)\n",
    "print(f\"LPIPS score: {score.item()}\")\n",
    "\n",
    "img1 = transform(transf_image).unsqueeze(0)\n",
    "img2 = transform(result2).unsqueeze(0)\n",
    "score = loss_fn(img1, img2)\n",
    "print(f\"LPIPS score: {score.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_sbi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
